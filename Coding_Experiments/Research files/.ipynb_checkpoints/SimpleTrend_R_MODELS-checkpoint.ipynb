{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Trend-Following strategy\n",
    "\n",
    "## Background\n",
    "\n",
    "Applying a Simple Tren-Following strategy, where trades are made based on the direction of recent price movements. In this specific case, the strategy is based on the percentage change in the OHLC (open, high, low, close) prices, with a buy signal generated if the percentage change is positive and a sell signal generated if the percentage change is negative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../functions_library/')\n",
    "\n",
    "from functions import ROC, model_selection, Backtesting\n",
    "\n",
    "#import sklearn \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "import sklearn.ensemble\n",
    "\n",
    "# Classification Metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc #plot_roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# ML models \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "\n",
    "# libraries for Shapely analysis\n",
    "import shap \n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV file into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the OHLCV dataset into a Pandas Dataframe\n",
    "trading_df = pd.read_csv(\n",
    "    Path(\"../Resources/ETHUSDT-1h-data.csv\"), \n",
    "    index_col=\"timestamp\", \n",
    "    infer_datetime_format=True, \n",
    "    parse_dates=True\n",
    ")\n",
    "\n",
    "trading_df = trading_df.drop([\"close_time\",\"quote_av\",\"trades\",\"tb_base_av\",\"tb_quote_av\",\"ignore\"], axis =1)\n",
    "\n",
    "# Review the DataFrame\n",
    "trading_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a daily return values column to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the daily returns using the closing prices and the pct_change function\n",
    "trading_df[\"actual_returns\"] = trading_df[\"close\"].pct_change()\n",
    "\n",
    "# Drop all NaN values from the DataFrame\n",
    "trading_df = trading_df.dropna()\n",
    "\n",
    "# Review the DataFrame\n",
    "display(trading_df.head())\n",
    "display(trading_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the features set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Adding more Features, defining test and training dataset, and more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import finta as ft\n",
    "from finta import TA\n",
    "import talib\n",
    "\n",
    "ohlcv_df = trading_df\n",
    "\n",
    "# List of time periods to use for Moving Averages calculation\n",
    "timeperiods = [5,7,14,20,30,50,70,100,150,200]\n",
    "\n",
    "df = ohlcv_df.copy()\n",
    "\n",
    "# Calculate SMAs and add them to the DataFrame\n",
    "for t in timeperiods:\n",
    "    #tsma = TA.SMA(df, t).shift(1)\n",
    "    sma = TA.SMA(df, t)\n",
    "    ema = TA.EMA(df, t)\n",
    "    atr = TA.ATR(df, t)  #Average True Range\n",
    "    adx = TA.ADX(df, t) \n",
    "    rsi = TA.RSI(df, t)\n",
    "    hma = TA.HMA(df, t)\n",
    "    vama = TA.VAMA(df, t)\n",
    " \n",
    "    # calculate the Force Index\n",
    "    force_index = pd.Series(df['close'].diff(1) * df['volume'], index=df.index)\n",
    "    force_ema = force_index.ewm(span=t, min_periods=0, adjust=True, ignore_na=False).mean()    \n",
    "    \n",
    "    #df['force_index'] = force_index\n",
    "    #df[f'force_index_ema_{t}'] = force_ema # add the Force Index and its EMA to the DataFrame\n",
    "    #df[f'TSMA_{t}'] = tsma\n",
    "    df[f'SMA_{t}'] = sma\n",
    "    df[f'EMA_{t}'] = ema\n",
    "    df[f'HMA_{t}'] = hma\n",
    "    df[f'VAMA_{t}'] = vama\n",
    "    df[f'ATR_{t}'] = atr\n",
    "    df[f'ADX_{t}'] = adx\n",
    "    df[f'RSI_{t}'] = rsi\n",
    "    \n",
    "    \n",
    "# Calculate the Parabolic SAR\n",
    "#sar = TA.PSAR(df)\n",
    "\n",
    "# Add the SAR values and trend direction to the DataFrame\n",
    "#df['sar'] = sar['psar']\n",
    "#df['psarbear'] = sar['psarbear']\n",
    "#df['psarbull'] = sar['psarbull']\n",
    "\n",
    "df['UO'] = TA.UO(df)\n",
    "\n",
    "# Adding Awesome Indicator (AO)\n",
    "df['AO'] = TA.AO(df)\n",
    "df['OBV'] =TA.OBV(df)\n",
    "\n",
    "# Adding Chaikin Indicator \n",
    "df['CHAIKIN'] = TA.CHAIKIN(df)\n",
    "\n",
    "# Adding Bollinger Bands\n",
    "df[['BB_UPPER','BB_MED','BB_LOWER']] =TA.BBANDS(df)\n",
    "\n",
    "# Calculate the Keltner Channel with TALIB\n",
    "#df[['KC_UPPER','KC_MED','KC_LOWER']] = TA.KC(df)\n",
    "\n",
    "# calculate Commodity Channel Index (CCI)\n",
    "df['cci'] = TA.CCI(df)\n",
    "\n",
    "# assuming you have OHLCV data in a pandas dataframe called \"df\"\n",
    "#volume_momentum = talib.MOM(df['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# calculate the Ichimoku Kinko Hyo indicator\n",
    "# Calculate the conversion line\n",
    "nine_period_high = df['high'].rolling(window=9).max()\n",
    "nine_period_low = df['low'].rolling(window=9).min()\n",
    "df['tenkan_sen'] = (nine_period_high + nine_period_low) / 2\n",
    "\n",
    "# Calculate the base line\n",
    "periods = 26\n",
    "twenty_six_period_high = df['high'].rolling(window=periods).max()\n",
    "twenty_six_period_low = df['low'].rolling(window=periods).min()\n",
    "df['kijun_sen'] = (twenty_six_period_high + twenty_six_period_low) / 2\n",
    "\n",
    "# Calculate the leading span A\n",
    "df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(periods=periods)\n",
    "\n",
    "# Calculate the leading span B\n",
    "periods2 = 52\n",
    "fifty_two_period_high = df['high'].rolling(window=periods2).max()\n",
    "fifty_two_period_low = df['low'].rolling(window=periods2).min()\n",
    "df['senkou_span_b'] = ((fifty_two_period_high + fifty_two_period_low) / 2).shift(periods=periods)\n",
    "\n",
    "# Calculate the lagging span\n",
    "df['chikou_span'] = df['close'].shift(periods=-periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a daily return values column to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the daily returns using the closing prices and the pct_change function\n",
    "df[\"actual_returns\"] = df[\"close\"].pct_change()\n",
    "\n",
    "# Drop all NaN values from the DataFrame\n",
    "df = df.dropna()\n",
    "\n",
    "# Review the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column in the trading_df called signal setting its value to zero.\n",
    "df[\"signal\"] = 0.0\n",
    "\n",
    "# Create the signal to buy\n",
    "df.loc[(df[\"actual_returns\"] >= 0), \"signal\"] = 1\n",
    "\n",
    "# Create the signal to sell\n",
    "df.loc[(df[\"actual_returns\"] < 0), \"signal\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"signal\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dealing with Class Imbalance using Undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Count the number of samples in each class\n",
    "class_counts = df['signal'].value_counts()\n",
    "\n",
    "# Find the class with fewer samples\n",
    "minority_class = class_counts.idxmin()\n",
    "\n",
    "# Split the dataframe into the majority and minority classes\n",
    "majority_class = df[df['signal'] != minority_class]\n",
    "minority_class = df[df['signal'] == minority_class]\n",
    "\n",
    "# Undersample the majority class to match the number of samples in the minority class\n",
    "undersampled_majority = resample(majority_class,\n",
    "                                 replace=False,\n",
    "                                 n_samples=len(minority_class),\n",
    "                                 random_state=42)\n",
    "\n",
    "# Combine the undersampled majority class with the minority class\n",
    "balanced_df = pd.concat([undersampled_majority, minority_class])\n",
    "\n",
    "# Shuffle the rows in the balanced dataframe\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "balanced_df[\"signal\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Generating the Feauture and Target Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column in the trading_df called signal setting its value to zero.\n",
    "X= balanced_df.copy()\n",
    "X= X.drop([\"open\",\"high\",\"low\",\"close\",\"volume\",\"actual_returns\",\"signal\"], axis =1).shift().dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the target set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy the new signal column to a new Series called y.\n",
    "y = balanced_df[\"signal\"].copy()\n",
    "\n",
    "# keeping y and X the same size \n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Use RFE to keep the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "\n",
    "# Create an RFE model to select the best features\n",
    "rfe = RFE(model, n_features_to_select=15)\n",
    "\n",
    "# Fit the RFE model to the data\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "# keep only specified columns in the dataframe\n",
    "#X = X.iloc[:, selected_features]\n",
    "X = X[selected_features]\n",
    "\n",
    "# Print the selected features\n",
    "print (\"Number of selected features:\", len(X.columns), \"\\nSelected features:\\n\",selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Remove highly-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_feats = X.copy()\n",
    "# Calculate correlation matrix\n",
    "# calculate the correlation matrix of the features\n",
    "corr_matrix = df_feats.corr()\n",
    "\n",
    "# set the threshold for correlation value\n",
    "corr_threshold = 0.90\n",
    "\n",
    "# find the highly correlated features and drop them from the dataframe\n",
    "high_corr_features = np.where(corr_matrix.abs() > corr_threshold)\n",
    "high_corr_features = [(corr_matrix.columns[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_features) if x != y and x < y]\n",
    "df_feats.drop([col[1] for col in high_corr_features], axis=1, inplace=True)\n",
    "\n",
    "# select the remaining features with low correlation\n",
    "low_corr_features = df_feats.columns.tolist()\n",
    "\n",
    "# print the low correlated features\n",
    "print(low_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "# calculate the correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# get the upper triangle of the correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "# get the index of the features with the lowest correlation\n",
    "to_keep = [column for column in upper.columns if any(upper[column] < 0.9)]\n",
    "\n",
    "# keep only the features with the lowest correlation\n",
    "X = X[to_keep]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X = X[low_corr_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "''' keeping note of the best selected features: \n",
    "Selected features:\n",
    " Index(['EMA_5', 'HMA_5', 'RSI_5', 'HMA_14', 'VAMA_20', 'RSI_30', 'HMA_50',\n",
    "       'ATR_50', 'RSI_50', 'HMA_70', 'VAMA_70', 'RSI_70', 'RSI_100',\n",
    "       'tenkan_sen', 'chikou_span'],\n",
    "      dtype='object')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Splitting the data and building the testing and training set using a timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# Select the start of the training period\n",
    "training_begin = X.index.min()\n",
    "\n",
    "# Display the training begin date\n",
    "#print(training_begin)\n",
    "\n",
    "# Select the ending period for the training data with an offset of 36 months\n",
    "training_end = X.index.min() + DateOffset(months=24)\n",
    "\n",
    "# Display the training end date\n",
    "#print(training_end)\n",
    "\n",
    "# Generate the X_train and y_train DataFrames\n",
    "X_train = X.loc[training_begin:training_end]\n",
    "y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "# Display sample data\n",
    "display(X_train.head())\n",
    "\n",
    "# Generate the X_test and y_test DataFrames\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]\n",
    "\n",
    "# Display sample data\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    " \n",
    "# Apply the scaler model to fit the X-train data\n",
    "X_scaler = scaler.fit(X_train)\n",
    " \n",
    "# Transform the X_train and X_test DataFrames using the X_scaler\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Choose the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_df = model_selection(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Incorporate the xgboost Machine Learning Into the Trading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#from xgboost import xgbClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#replace -1 by 0 as xgboost expects a boolean target vector (only 0 and 1)\n",
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)\n",
    "\n",
    "# Create a xgb Classifier model\n",
    "xgb_clf = xgb.XGBClassifier(reg_alpha=27)\n",
    "\n",
    "xgb_clf.fit(X_train_scaled, y_train) \n",
    "\n",
    "# Use the best model to make predictions on the test data\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "\n",
    "ROC(xgb_clf,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the trading fee as a decimal\n",
    "trading_fee = 0.00075\n",
    "classifier = xgb_clf\n",
    "\n",
    "# Backtest using our exteranl function\n",
    "xgb_predictions_df = Backtesting (df, X_test, X_test_scaled, classifier,trading_fee)\n",
    "\n",
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + xgb_predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create the classifier model\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create NuSVC classifier\n",
    "svm_model = NuSVC(nu=0.5, probability= True, kernel='rbf')\n",
    "\n",
    "# Fit the model to the data using X_train_scaled and y_train\n",
    "svm_model = svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "ROC(svm_model,X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Adding CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from catboost import CatBoostClassifier\n",
    "# Initialize the CatBoost model\n",
    "\n",
    "cb_clf = CatBoostClassifier(iterations=450,random_state=seed  ,l2_leaf_reg=55.8, verbose = False)\n",
    "\n",
    "# Train the model on the training data\n",
    "cb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# display ROC and classification metrics \n",
    "ROC(cb_clf,X_train_scaled, X_test_scaled, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the trading fee as a decimal\n",
    "trading_fee = 0.00075\n",
    "classifier = cb_clf\n",
    "\n",
    "# Backtest using our exteranl function\n",
    "cb_predictions_df = Backtesting (df, X_test, X_test_scaled, classifier,trading_fee)\n",
    "\n",
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + cb_predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# create LDA object and fit the model\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Train the model on the training data\n",
    "lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "ROC(lda,X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the trading fee as a decimal\n",
    "trading_fee = 0.00075\n",
    "classifier = lda\n",
    "\n",
    "# Backtest using our exteranl function\n",
    "lda_predictions_df = Backtesting (df, X_test, X_test_scaled, classifier,trading_fee)\n",
    "\n",
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + lda_predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the trained model to predict the trading signals for the testing data.\n",
    "testing_signal_predictions = cb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Create a new empty predictions DataFrame using code provided below.\n",
    "predictions_df = pd.DataFrame(index=X_test.index)\n",
    "predictions_df[\"predicted_signal\"] = testing_signal_predictions\n",
    "predictions_df[\"actual_returns\"] = trading_df[\"actual_returns\"]\n",
    "predictions_df[\"trading_algorithm_returns\"] = predictions_df[\"actual_returns\"] * predictions_df[\"predicted_signal\"]\n",
    "\n",
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_selection(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resample X_test\n",
    "upscaled_X_test = X_test.resample('1D').interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define which classifier to use \n",
    "clf = xgb_clf\n",
    "\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(clf.predict, upscaled_X_test)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(upscaled_X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, upscaled_X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAP Analysis \n",
    "# define which classifier to use \n",
    "clf = cb_clf\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(clf.predict, upscaled_X_test)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(upscaled_X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, upscaled_X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier model\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create NuSVC classifier\n",
    "svm_model = NuSVC(nu=0.15, probability= True, kernel='rbf')\n",
    "\n",
    "# Fit the model to the data using X_train_scaled and y_train\n",
    "svm_model = svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "ROC(svm_model,X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trading fee as a decimal\n",
    "trading_fee = 0.00075\n",
    "classifier = svm_model\n",
    "\n",
    "# Backtest using our exteranl function\n",
    "svm_predictions_df = Backtesting (df, X_test, X_test_scaled, classifier,trading_fee)\n",
    "\n",
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + svm_predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev310",
   "language": "python",
   "name": "dev310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
